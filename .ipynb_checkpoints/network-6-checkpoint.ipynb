{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: Check gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    data : (10 000 x 3072)\n",
    "    labels : (10 000 x 1)\n",
    "    one_hot : (10 000 x 10)\n",
    "    \"\"\"\n",
    "    \n",
    "    file = unpickle(\"data/cifar-10-batches-py/data_batch_1\")\n",
    "    labels = file[b'labels']\n",
    "    data = file[b'data']\n",
    "    no_classes = 10\n",
    "    N = len(labels)\n",
    "    \n",
    "    one_hot = np.zeros((N, no_classes))\n",
    "    one_hot[np.arange(N), labels] = 1\n",
    "    \n",
    "    labels = np.array(labels).reshape(-1,1)\n",
    "    \n",
    "    # normalize\n",
    "    mean = np.mean(data, axis=0, keepdims=True)\n",
    "    std = np.std(data, axis=0, keepdims=True)\n",
    "    \n",
    "    X = (data - mean) / std\n",
    "    return X, labels, one_hot, mean, std\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.where(x > 0, x, 0)\n",
    "\n",
    "def dReLU(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def plot_image(x, mean, std):\n",
    "    x = (x.T * std + mean).astype(int)\n",
    "    img = x.reshape(3,32,32)\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.imshow(np.transpose(img, (1,2,0)))\n",
    "\n",
    "def check_if_correct(y, p):\n",
    "    temp = np.argmax(y, axis=1) - np.argmax(p.T, axis=1)\n",
    "    correct_ones = np.where(temp == 0, 1, 0)\n",
    "    return np.sum(correct_ones)\n",
    "\n",
    "def print_info(plotter, epoch, avg_loss, accuracy):\n",
    "    if (epoch % 10 == 0):\n",
    "        plotter.add(epoch, avg_loss)\n",
    "        print(\"epoch: {} \\tloss: {:.3} \\tacc: {:.3}\".format(epoch, avg_loss, accuracy))\n",
    "\n",
    "class Plotter:\n",
    "    def __init__(self, title):\n",
    "        self.title = title\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        \n",
    "    def add(self, epoch, cost):\n",
    "        self.y.append(cost)\n",
    "        self.x.append(epoch)\n",
    "    \n",
    "    def plot(self):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(self.x, self.y)\n",
    "\n",
    "        ax.set(xlabel=\"epochs\", ylabel=\"cost\", title=self.title)\n",
    "        ax.grid()\n",
    "        # fig.savefig(\"{}.png\".format(self.title))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "w_decay = 0\n",
    "lr = 0.05\n",
    "batch_norm = False\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, in_size, out_size):\n",
    "        self.W = np.random.rand(out_size, in_size) * 0.0001\n",
    "        self.b = np.zeros((out_size, 1))\n",
    "        \n",
    "        if batch_norm:\n",
    "            self.gamma = np.random.rand(out_size, 1)\n",
    "            self.beta = np.zeros((out_size, 1))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if batch_norm:\n",
    "            self.input = input\n",
    "            self.S = np.dot(self.W, input) + self.b\n",
    "            self.mu = np.sum(self.S, axis=1).reshape(-1,1) / batch_size\n",
    "            self.var = np.var(self.S, axis=1)\n",
    "\n",
    "            self.S_hat = np.dot(np.diag( (self.var + 1e-15) ** (-0.5) ), (self.S - self.mu))\n",
    "            self.S_t = self.gamma * self.S_hat + self.beta\n",
    "\n",
    "            self.output = ReLU(self.S_t)\n",
    "            return self.output\n",
    "        else:\n",
    "            self.input = input\n",
    "            self.S = np.dot(self.W, input) + self.b\n",
    "            self.output = ReLU(self.S)\n",
    "            return self.output\n",
    "    \n",
    "    def backprop(self, G):\n",
    "        G = G * dReLU(self.output)\n",
    "        \n",
    "        if batch_norm:\n",
    "            self.dgamma = np.sum(G * self.S_hat, axis=1, keepdims=True) / batch_size\n",
    "            self.dbeta = np.sum(G, axis=1, keepdims=True) / batch_size\n",
    "\n",
    "            G = G * self.gamma\n",
    "\n",
    "            sigma = self.var.reshape(-1,1) + 1e-15\n",
    "            G1 = G * (sigma ** -0.5)\n",
    "            G2 = G * (sigma ** -1.5)\n",
    "\n",
    "            D = self.S - self.mu\n",
    "            c = np.sum(G2 * D, axis=1, keepdims=True)\n",
    "            part1 = (1 / batch_size) * (G1 @ np.ones((batch_size, 1))) @ np.ones((1, batch_size))\n",
    "            part2 = (1 / batch_size) * D * (c @ np.ones((1, batch_size)))\n",
    "\n",
    "            G = G1 - part1 - part2\n",
    "\n",
    "            self.dW = (np.dot(G, self.input.T) / batch_size) + w_decay * 2 * self.W\n",
    "            self.db = np.sum(G, axis=1).reshape(-1,1) / batch_size\n",
    "\n",
    "            return np.dot(self.W.T, G)\n",
    "        else:\n",
    "            self.dW = (np.dot(G, self.input.T) / batch_size) + w_decay * 2 * self.W\n",
    "            self.db = np.sum(G, axis=1).reshape(-1,1) / batch_size\n",
    "            \n",
    "            return np.dot(self.W.T, G)\n",
    "    \n",
    "    def update(self):\n",
    "        self.W = self.W - lr * self.dW\n",
    "        self.b = self.b - lr * self.b\n",
    "        \n",
    "        if batch_norm:\n",
    "            self.gamma = self.gamma - lr * self.dgamma\n",
    "            self.beta = self.beta - lr * self.dbeta\n",
    "        \n",
    "class LastLayer:\n",
    "    def __init__(self, in_size, no_classes):\n",
    "        self.W = np.random.rand(no_classes, in_size) * 0.0001\n",
    "        self.b = np.zeros((no_classes, 1))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        S = np.dot(self.W, input) + self.b\n",
    "        self.output = softmax(S)\n",
    "        return self.output\n",
    "    \n",
    "    def backprop(self, G):\n",
    "        self.dW = (np.dot(G, self.input.T) / batch_size) + 2 * w_decay * self.W\n",
    "        self.db = np.sum(G, axis=1).reshape(-1,1) / batch_size\n",
    "        \n",
    "        return np.dot(self.W.T, G)\n",
    "\n",
    "    def update(self):\n",
    "        self.W = self.W - lr * self.dW\n",
    "        self.b = self.b - lr * self.b\n",
    "\n",
    "def build_network(sizes):\n",
    "    layers = []\n",
    "    for idx in range(len(sizes) - 2):\n",
    "        layers.append(Layer(sizes[idx], sizes[idx+1]))\n",
    "\n",
    "    layers.append(LastLayer(sizes[-2], sizes[-1]))    \n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_error 0.9900982572602505\n",
      "1.1538402655154836e-08 1.1652900866465643e-06\n",
      "0.185 s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(44)\n",
    "\n",
    "def numerical_grads(layers, X, one_hot):\n",
    "    X_temp = X.copy()\n",
    "    \n",
    "    dW0 = layers[0].dW\n",
    "    dW1 = layers[1].dW\n",
    "    db0 = layers[0].db\n",
    "    db1 = layers[1].db\n",
    "    \n",
    "    dW0_num = np.zeros(dW0.shape)\n",
    "    new_W0 = layers[0].W.copy()\n",
    "    \n",
    "    h = 1e-6\n",
    "    max_error = 0\n",
    "    back = 0\n",
    "    num = 0\n",
    "    for (x,y), _ in np.ndenumerate(new_W0):\n",
    "        W_temp = new_W0.copy()\n",
    "        W_temp[x,y] += h\n",
    "        layers[0].W = W_temp\n",
    "        c1, _ = calc_loss(layers, X, one_hot)\n",
    "        \n",
    "        W_temp = new_W0.copy()\n",
    "        W_temp[x,y] -= h\n",
    "        layers[0].W = W_temp\n",
    "        c2, _ = calc_loss(layers, X, one_hot)\n",
    "\n",
    "        grad = -(c2 - c1) / (2 * h)\n",
    "        # dW0_num[x,y] = grad\n",
    "        \n",
    "        err = np.abs(dW0[x,y] - grad) / np.maximum(dW0[x,y], grad)\n",
    "        if err > max_error:\n",
    "            max_error = err\n",
    "            back = dW0[x,y]\n",
    "            num = grad\n",
    "    \n",
    "    print(\"max_error\", max_error)\n",
    "    print(back, num)\n",
    "\n",
    "    \n",
    "    \n",
    "def calc_loss(layers, X, one_hot):\n",
    "    X_temp = X.copy()\n",
    "    \n",
    "    for layer in layers:\n",
    "        X_temp = layer.forward(X_temp)\n",
    "        \n",
    "    L2 = 0\n",
    "    for layer in layers:\n",
    "        L2 += np.sum(layer.W ** 2)\n",
    "    \n",
    "    loss = -np.sum(one_hot * np.log(1e-15 + X_temp.T)) + w_decay * L2\n",
    "    return loss, X_temp\n",
    "\n",
    "def test_gradients():\n",
    "    data, _, one_hots, mean, std = load_data()\n",
    "    data_size = data.shape[0]\n",
    "    data = data[:, :40]\n",
    "    \n",
    "    layers = build_network([40, 20, 10])\n",
    "    iterations = data_size // batch_size\n",
    "    start_t = time.time()\n",
    "    \n",
    "    for epoch in range(1):        \n",
    "        for idx in range(iterations):\n",
    "            start = batch_size * idx\n",
    "            end = batch_size * (idx + 1)\n",
    "            \n",
    "            # get X and Y\n",
    "            X = data[start:end,:].T # 3072x100\n",
    "            one_hot = one_hots[start:end,:] # 100x10\n",
    "            \n",
    "            _, output = calc_loss(layers, X, one_hot)\n",
    "            \n",
    "            # Backprop\n",
    "            G = output - one_hot.T\n",
    "            \n",
    "            for layer in reversed(layers):\n",
    "                G = layer.backprop(G)\n",
    "            \n",
    "            # numerical\n",
    "            numerical_grads(layers, X, one_hot)\n",
    "            \n",
    "            break\n",
    "            \n",
    "            # Update\n",
    "            # for layer in reversed(layers):\n",
    "            #     layer.update()\n",
    "\n",
    "    print(\"{:.3} s\".format(time.time() - start_t))\n",
    "        \n",
    "test_gradients()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
